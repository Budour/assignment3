{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "version": "2.7.11", 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "<h1 align=\"center\">CS340 - Assignment 3</h1>\n<h3 align=\"center\">Due Date: 30 April 2017</h3>\n<br>\n<p style=\"text-indent: 40px\">In this project we are going to use a clustering method(LDA) on the Yelp dataset. The dataset is included in the assignment folder. You should unzip and upload the file to ibm's datascience tool.\n</p>\n\n\n##\u00a0Topics From Reviews\n\n\"LDA is a topic model which infers topics from a collection of text documents.\"\nFor the purpose of this assignment we can treat each yelp review as a document and extract two topics from the data by using LDA.\n\nFor this assignment you should use ml transformers and ml pipeline. Just for convenince, ReviewsRdd is provided for you. You should convert this rdd to DataFrame and work on it. In order to do this assignment you should revisit PySpark ML documentation many times.\n\nEnd result should look like this.\n\n<div>\n    <img src=\"http://image.prntscr.com/image/13eb8a01533346c0bb3186dfc5402b73.png\" width=200>\n</div>\n\nYour solution does not have to be exactly like this, since each time you run the LDA model, it gives you different results. \n\n<p style=\"text-indent: 40px\"> Note: You should remove stopwords with a feature transformer, the words you should remove are in the stopwords list that we have provided.</p>\n<p style=\"text-indent: 40px\">Important: Do not discuss the solution with your friends. <b>Plagiarism</b> will not be tolerated and issue will be referred to the <b>disciplinary committee</b>.</p>\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 1, 
            "outputs": [], 
            "source": "import pyspark\nsc = pyspark.SparkContext('local[*]')\nspark = pyspark.sql.SparkSession(sc)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": 2, 
            "outputs": [], 
            "source": "stopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\",\"i'll\",\"you'll\",\"he'll\",\"she'll\",\"we'll\",\"they'll\",\"i'd\",\"you'd\",\"he'd\",\"she'd\",\"we'd\",\"they'd\",\"i'm\",\"you're\",\"he's\",\"she's\",\"it's\",\"we're\",\"they're\",\"i've\",\"we've\",\"you've\",\"they've\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"don't\",\"doesn't\",\"didn't\",\"won't\",\"wouldn't\",\"shan't\",\"shouldn't\",\"mustn't\",\"can't\",\"couldn't\",\"cannot\",\"could\",\"here's\",\"how's\",\"let's\",\"ought\",\"that's\",\"there's\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\"]\n\n# convert all words to lowercase\n# remove words that are too long or too short \nreviewsRdd = sc.textFile('/data/Spark/assignment3/100kReviews.txt')\\\n            .map(lambda line: line.lower())\\\n            .map(lambda line: \" \".join(word for word in line.strip().split() if 25 > len(word) > 4))", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": 3, 
            "outputs": [], 
            "source": "reviewsDf = reviewsRdd.map(lambda x: pyspark.sql.types.Row(x)).toDF(['comment'])", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": 4, 
            "outputs": [], 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import RegexTokenizer,StopWordsRemover,CountVectorizer\n\ntokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"words\", pattern=\"\\\\W\")\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"processed\", stopWords=stopwords)\nvectorizer = CountVectorizer(inputCol=\"processed\", outputCol=\"features\", vocabSize=10)\n\npipeline = Pipeline().setStages([tokenizer,remover,vectorizer]).fit(reviewsDf)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": 6, 
            "outputs": [], 
            "source": "from pyspark.mllib.linalg import Vectors\n\nfeatureRdd = pipeline.transform(reviewsDf).select(\"features\").rdd.map(list)\ncorpus = featureRdd.map(lambda x: Vectors.dense(x[0])).zipWithIndex().map(lambda x:[x[1],x[0]]).cache()", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": 9, 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Topic:0\nplace\ngreat\nreally\nservice\nstaff\nalways\npeople\nlittle\nfriendly\nstore\n\nTopic:1\nplace\ngreat\nreally\nservice\nstaff\nalways\npeople\nlittle\nfriendly\nstore\n\n"
                }
            ], 
            "source": "from pyspark.mllib.clustering import LDA, LDAModel\nldaModel = LDA.train(corpus, k=2)\ntopicDescription = ldaModel.describeTopics()\nwordDictionary = dict(zip(range(len(pipeline.stages[2].vocabulary)),pipeline.stages[2].vocabulary))\nfor i in range(2):\n    print \"Topic:\"+str(i)\n    for index in topicDescription[i][0]:\n        print wordDictionary[index]\n    print \"\"", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code"
        }
    ]
}